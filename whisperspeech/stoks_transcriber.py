# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/5B. Stoks transcriber.ipynb.

# %% auto 0
__all__ = ['load_dataset', 'rand', 'Tunables', 'T2SEmbedding', 'Encoder', 'STARTransformer', 'make_model']

# %% ../nbs/5B. Stoks transcriber.ipynb 1
import dataclasses
import random
import math
import itertools
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.profiler import record_function

from huggingface_hub import hf_hub_download
from fastcore.basics import store_attr
from fastprogress import progress_bar

from pathlib import Path

# %% ../nbs/5B. Stoks transcriber.ipynb 2
from whisperspeech.modules import *
from whisperspeech import languages, inference, up_initialization
import sentencepiece as spm
import webdataset as wds

# %% ../nbs/5B. Stoks transcriber.ipynb 6
import re

class CharTokenizer:
    """Trivial tokenizer â€“ just use UTF-8 bytes"""
    eot = 0
    
    def encode(self, txt):
        return list(bytes(txt.strip(), 'utf-8'))

    def decode(self, tokens):
        return bytes(tokens).decode('utf-8')

def tokenizer(ikey, okey, length):
    """Tokenizes a transcript"""
    tok = CharTokenizer()
    def _tokenizer(samples):
        for s in samples:
            toks = torch.tensor(tok.encode(s[ikey]))
            s[okey] = F.pad(toks, (0, length - toks.shape[-1]), value=tok.eot)
            yield s
    return _tokenizer

def bpe_tokenizer(ikey, okey, sp, length):
    def _tokenizer(samples):
        for s in samples:
            toks = torch.tensor(sp.tokenize(s[ikey]))
            if len(toks) > length:
                continue
            s[okey] = F.pad(toks, (0, length - toks.shape[-1]), value=sp.pad_id())
            s['n_'+okey] = toks.shape[-1]
            yield s
    return _tokenizer

def ar_padder(ikey, okey, length, pad_token):
    """Pads the tokens for autoregresive training"""
    import numpy as np

    def _ar_padder(samples):
        for s in samples:
            toks = s[ikey]
            if isinstance(toks, (list, np.ndarray)): toks = torch.tensor(toks)
            toks = toks.to(torch.long)
            s['in_' +okey] = F.pad(toks, (1, length - toks.shape[-1] - 1), value=pad_token)
            s['out_'+okey] = F.pad(toks, (0, length - toks.shape[-1]), value=pad_token)
            if 'n_'+okey not in s: s['n_'+okey] = toks.shape[-1]
            yield s
    return _ar_padder

# %% ../nbs/5B. Stoks transcriber.ipynb 7
def load_dataset(
    dataset_dir:Path,
    stoks_dir:str=None,
    txt_dir:str=None,
    bpe_model:str='txts-250.model',
    vq_codes:int=4096,
    weight:float=1,
    validation:bool=False,
    override_language:str=None,
    randomize_cps:bool=False,
    exclude_datasets:str="txt-random-valid",
):
    import webdataset as wds
    from whisperspeech import utils, languages

    dataset_dir = Path(dataset_dir)
    
    if txt_dir is None:
        for name in ['small.en-txt', 'medium-txt']:
            if (dataset_dir/name).exists():
                txt_dir = name
                break
    assert txt_dir is not None, f"No transcripts found in {dataset_dir}"

    txt_path = dataset_dir/f'{txt_dir}/*.tar.gz'
    shards = utils.shard_glob(txt_path)
    assert len(shards), f"No data shards found in {txt_path}."

    with open(dataset_dir/'txt-samples.list') as f: samples = len(f.readlines())
    language = override_language or utils.readlines(dataset_dir/'language')[0]
    language = languages.to_id(language)
    
    excludes = {x
                for dir in exclude_datasets.split()
                for x in utils.readlines(dataset_dir/Path(dir)/"txt-samples.list")
               } if not validation and exclude_datasets else set()

    def set_language(x):
        x['language'] = language
        return x

    sp = spm.SentencePieceProcessor(model_file=bpe_model)
    
    same_on_all_nodes = lambda urls: urls # will only be used for validation
    ds = wds.WebDataset(shards, resampled=not validation, nodesplitter=same_on_all_nodes).compose(
        wds.decode(),
        utils.merge_in(utils.derived_dataset(stoks_dir)),
        wds.select(lambda s: s['__key__'] not in excludes and len(s['stoks.npy']) > 0), # discard validation samples
        bpe_tokenizer(sp, 'txt', 'ttoks', sp, length=500),
        ar_padder('stoks.npy', 'stoks', length=750, pad_token=vq_codes-1),
        ar_padder('ttoks', 'ttoks', length=500, pad_token=-1),
        # char_per_seconder('txt', 'stoks.npy', 'cps', stoks_per_second=25),
        # wds.map(set_language),
        wds.to_tuple('in_stoks', 'out_stoks', 'n_stoks', 'in_ttoks', 'out_ttoks', 'n_ttoks'),
    )
    if validation:
        ds = ds.compose(
            wds.batched(samples)
        ).slice(1)
        if randomize_cps:
            rng = np.random.default_rng()
            ds = ds.compose(
                wds.map_tuple(None, None, None, lambda x: rng.permutation(x), None, None),
        )
    else:
        ds = ds.compose(
            # wds.shuffle(20000, initial=20000),
            # wds.batched(2048)
        )
    ds.total_samples = samples
    ds.stoks_len = 750
    ds.stoks_codes = vq_codes
    ds.ttoks_codes = sp.vocab_size()
    ds.tokenizer = sp
    ds.ttoks_len = 500
    ds.weight = weight

    return ds

# %% ../nbs/5B. Stoks transcriber.ipynb 34
def rand(start, end):
    return random.random() * (end - start) + start

@dataclasses.dataclass
class Tunables:
    init_std :float = 1
    embeddings_std :float = .01
    embeddings_lr_scale: float = 5
    embedding_projector_lr_scale: float = 2.5
    output_mult :float = .35
    query_mult :float = 1
    encoder_depth_ratio :float = 0.25
    causal_encoder: bool = True
    eot_dropout_p :float = .5
    cps_input: bool = True
    cps_bins: int = 32
    padding_token_offset: int = 0
        
    lr0 :float = 1.5e-3
    clip_gradient_norm :float = .2
    weight_decay :float = 1e-1
    warmup_steps :float = 4000

    random :bool = False

    def __post_init__(self):
        # randomize the hyperparams if requested
        if self.random:
            self.init_std = 10**rand(-1,1)
            self.embeddings_std = 10**rand(-3,-.7)
            self.embeddings_lr_scale = rand(2,6)
            self.output_mult = rand(0.25,0.65)
            self.query_mult = 2**rand(-2,3)
            self.encoder_depth_ratio = 0.25
            
            self.lr0 = rand(1,5)*1e-3
            self.clip_gradient_norm = 10**rand(-3,0)
            self.warmup_steps = 100*(10**rand(1,1.85))

    @staticmethod
    def upgrade(args):
        args = {k:v for k,v in args.items()}
        def old_default(name, value):
            if name not in args: args[name] = value
        old_default('padding_token_offset', -1)
        return args

# %% ../nbs/5B. Stoks transcriber.ipynb 35
class T2SEmbedding(nn.Module):
    def __init__(self, length=1500, codes=1024, width=384, pos_embs=None, stoks_width=384):
        super().__init__()
        self.embedding = FlexEmbeddings(codes, width, special_codes=1, frozen_width=stoks_width)
        if pos_embs is None: pos_embs = sinusoids(length, width)
        self.register_buffer("positional_embedding", pos_embs)
    
    def forward(self, Stoks, xenc, cps=None, offset=0):
        Sembs = self.embedding(Stoks)
        xin = (Sembs + self.positional_embedding[offset : offset + Sembs.shape[1]]).to(xenc.dtype)
        if cps is not None: xin = xin + cps
        return xin, offset

# %% ../nbs/5B. Stoks transcriber.ipynb 36
class Encoder(nn.Module):
    def __init__(self, depth=6, width=384, n_head=6, length=1500, codes=1024, emb_width=384, ffn_mult=4, pos_embs=None, tunables=Tunables()):
        super().__init__()
        self.emb_width = emb_width
        self.tunables = tunables
        
        self.embedding = nn.Embedding(codes, width) #FlexEmbeddings(codes, width, frozen_width=emb_width)

        if pos_embs is None: pos_embs = sinusoids(length, width)
        self.register_buffer("positional_embedding", pos_embs)

        self.layers = nn.ModuleList([
            ResidualAttentionBlock(width, n_head,
                                   qk_scale=tunables.query_mult*8/math.sqrt(width/n_head), ffn_mult=ffn_mult) for _ in range(depth)
        ])

        self.ln_post = LayerNorm(width)
        
        mask = torch.empty(length, length).fill_(-torch.inf).triu_(1)
        self.register_buffer("mask", mask, persistent=False)

    def forward(self, Stoks, positions, lang_emb=None):
        xin = self.embedding(Stoks)

        # if lang_emb is not None: xin = xin + lang_emb
        
        x = (xin +
             self.positional_embedding[positions]).to(xin.dtype)

        for l in self.layers: x = l(x, positions,
                                    causal=self.tunables.causal_encoder and self.training,
                                    mask=self.mask if self.tunables.causal_encoder and not self.training else None)
        
        return self.ln_post(x)

# %% ../nbs/5B. Stoks transcriber.ipynb 37
class STARTransformer(nn.Module):
    def __init__(self, depth=6, n_head=6, head_width=64, ffn_mult=4,
                 ttoks_len=500, ttoks_codes=250, ttoks_width=None,
                 stoks_len=750, stoks_codes=513, stoks_width=None,
                 tokenizer=None,
                 tunables=Tunables()):
        super().__init__()
        store_attr("depth,n_head,head_width,ffn_mult,stoks_width,ttoks_width,ttoks_len,stoks_len,ttoks_codes,stoks_codes")

        width = n_head * head_width
        self.width = width
        self.base_width = 3 * head_width
        self.tunables = tunables
        if self.stoks_width is None: self.stoks_width = self.width
        if self.ttoks_width is None: self.ttoks_width = self.width
        self.ttoks_codes = ttoks_codes
        self.stoks_codes = stoks_codes

        self.tokenizer = tokenizer

        self.embedding = nn.Embedding(ttoks_codes+1, width)
        
        pos_embs = sinusoids(ttoks_len, width)
        self.register_buffer("positional_embedding", pos_embs)
        
        encoder_depth = int(depth * 2 * tunables.encoder_depth_ratio)
        decoder_depth = depth * 2 - encoder_depth
        tformer_args = dict(width=width, n_head=n_head, ffn_mult=ffn_mult, tunables=tunables)
        self.encoder = Encoder(length=stoks_len, codes=stoks_codes, emb_width=self.stoks_width, depth=encoder_depth, **tformer_args)

        self.decoder = BaseDecoder(
            length=stoks_len, 
            depth=decoder_depth,
            qk_scale=tunables.query_mult*8/math.sqrt(width/n_head),
            width=width, n_head=n_head, ffn_mult=ffn_mult,
        )
        
        self.apply(self.init_transformer)

    def setup(self, device):
        pass

    def init_transformer(self, m):
        up_initialization.init_transformer(self, m)
    
    def run_encoder(self, stoks):
        with record_function("encoder"):
            positions = torch.arange(0, stoks.shape[1], device=stoks.device)
            xenc = self.encoder(stoks.to(torch.long), positions)

        return xenc, positions
    
    def forward(self, in_stoks, out_stoks, n_stoks, in_ttoks, out_ttoks, n_ttoks, in_ttoks_positions=None, loss=True, offset=None, xenc=None, xenc_positions=None, cps_emb=None):
        if xenc is None:
            xenc, xenc_positions = self.run_encoder(in_stoks)

        with record_function("decoder"):
            if in_ttoks_positions is None: in_ttoks_positions = torch.arange(0, in_ttoks.shape[1], device=in_ttoks.device)

            x = (self.embedding(in_ttoks+1) + 
                 self.positional_embedding[in_ttoks_positions]).to(xenc[0].dtype)
            x = self.decoder(x, in_ttoks_positions, xenc.clone(), xenc_positions)
            # logits = self.embeddings.embedding.unembed(x)
            logits = (x @ self.embedding.weight.to(x.dtype).T).float()
            logits = logits * self.tunables.output_mult / (self.width / self.base_width)

        if loss is not None:
            with record_function("loss"):
                loss = F.cross_entropy(logits.transpose(-1,-2), out_ttoks+1)
                # if self.training and self.tunables.causal_encoder:
                #     enc_logits = self.encoder.embedding.unembed(xenc)
                #     enc_logits = enc_logits * self.tunables.output_mult / (self.width / self.base_width)
                #     loss += 0.1 * F.cross_entropy(enc_logits.transpose(-1,-2), out_ttoks)

        return logits, loss

    #
    # inference
    #
    @classmethod
    def load_model(cls, ref="gs://test-model-weights/whisperspeech/stoks_transcribe/timothy_white.model",
                   repo_id=None, filename=None, local_filename=None, spec=None, device=None):
        if spec is None:
            if ref.startswith('gs://'):
                local_filename = wds.cache.get_file_cached(ref)
            if repo_id is None and filename is None and local_filename is None:
                if ":" in ref:
                    repo_id, filename = ref.split(":", 1)
                else:
                    local_filename = ref
            if not local_filename:
                local_filename = hf_hub_download(repo_id=repo_id, filename=filename)
            spec = torch.load(local_filename, map_location=device)

        tokenizer = spec.get('tokenizer', None)
        if tokenizer: tokenizer = spm.SentencePieceProcessor(model_proto=tokenizer)
        
        model = cls(**spec['config'],
                    tunables=Tunables(**Tunables.upgrade(spec['tunables'])),
                    tokenizer=tokenizer)
        model.load_state_dict(spec['state_dict'])
        model.eval().to(device)
        return model

    def load_checkpoint(self, local_filename_or_obj):
        if isinstance(local_filename_or_obj, (str, Path)):
            spec = torch.load(local_filename, map_location='cpu')
        else:
            spec = local_filename_or_obj
        assert 'pytorch-lightning_version' in spec, 'not a valid PyTorch Lightning checkpoint'
        state_dict = {k.replace('model.', ''):v
                      for k,v in spec['state_dict'].items()}
        self.load_state_dict(state_dict)
        return self

    def save_model(self, fname):
        torch.save(dict(config = self.__stored_args__,
                        tunables = dataclasses.asdict(self.tunables),
                        state_dict = self.state_dict(),
                        tokenizer = self.tokenizer.serialized_model_proto()), fname)

    def switch_dtypes(self, dtype=torch.float16):
        self.dtype = dtype
        for n,m in self.named_modules():
            # convert every leaf layer apart from the LayerNorms
            if isinstance(m, (nn.Linear, nn.Embedding)):
                m.to(dtype)
            # take care of buffers ([kv]_cache, masks) that are not in the leaf layers
            for bn,b in m.named_buffers(recurse=False):
                setattr(m,bn,b.to(dtype))

    def optimize(self, max_batch_size=1, dtype=torch.float16, torch_compile=True):
        # for emb in [self.encoder.embedding, self.embedding]:
            # emb.convert_for_eval()
        for l in self.encoder.layers:
            l.attn.convert_for_eval()
        for l in self.decoder.layers:
            l.attn.convert_for_eval()
            l.cross_attn.convert_for_eval()
            l.setup_kv_cache(max_batch_size, self.ttoks_len, self.stoks_len)
        self.switch_dtypes(dtype)
        if torch_compile:
            self.generate_next = torch.compile(self.generate_next, mode="reduce-overhead", fullgraph=True)
            
    def optimize_training(self):
        # breaks with: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.
        # somewhere inside LayerNorm???
        self.encoder = torch.compile(self.encoder, fullgraph=True, mode="reduce-overhead")
        self.decoder = torch.compile(self.decoder, fullgraph=True, mode="reduce-overhead")

    @property
    def device(self):
        return next(self.parameters()).device

    def generate_one(self, toks, toks_positions, xenc, xenc_positions, T, top_k, repetition_penalty):
        probs, _ = self(None, None, None, toks, None, None, in_ttoks_positions=toks_positions, loss=None, xenc=xenc, xenc_positions=xenc_positions)
        probs = probs[:,-1]
        # print(probs)
        # probs[self.embedding.codes:] = -torch.inf
        # if repetition_penalty is not None:
            # probs.scatter_reduce_(1, toks[-25:-5], torch.full_like(toks[-25:-5], repetition_penalty, dtype=probs.dtype), reduce="sum")
        return inference.sample(probs, T, top_k)

    def generate_next(self, *args, **kwargs):
        return self.generate_one(*args, **kwargs)

    @torch.no_grad()
    def prep(self, txt, cps=15, lang="en"):
        dev = self.device
        ttoks = torch.tensor(self.tokenizer.encode(txt), device=dev)
        ttoks = F.pad(ttoks, (0, self.ttoks_len - len(ttoks)), value=self.tokenizer.eot).unsqueeze(0)
        cpss = torch.tensor([cps], device=dev)
        langs = torch.tensor([languages.to_id(lang)], device=dev)
        return ttoks, cpss, langs
    
    @torch.no_grad()
    def generate(self, stoks, ttoks_prompt=None, N=None, bs=1, T=0.7, top_k=None, repetition_penalty=None, step=None, show_progress_bar=True):
        N = N or self.ttoks_len
        dev = self.device
        stoks = torch.tensor(stoks, device=dev)
        stoks = F.pad(stoks, (1, self.stoks_len - len(stoks) - 1), value=self.stoks_codes-1)
        # print(stoks)
        T = torch.tensor(T, device=dev)

        toks = torch.zeros((bs,N), dtype=torch.long, device=dev)
        toks[:,0] = -1
        start = 0
        if ttoks_prompt is not None:
            toks[:,1:len(ttoks_prompt)+1] = ttoks_prompt
            start = len(ttoks_prompt)
        it = range(start+1,N-1)
        if show_progress_bar: it = progress_bar(it)

        toks_positions = torch.arange(N, device=dev)
        with record_function("encode"):
            stoks = stoks.repeat(bs, 1)
            xenc, xenc_positions = self.run_encoder(stoks)
            toks_positions = torch.arange(N+1, device=dev)
        
        with record_function("prefill"):
            # print('prefill:', self.generate_one(toks[:,:start+1].contiguous(), toks_positions[:start+1], xenc, xenc_positions, T, top_k, repetition_penalty))
            toks[:,start+1] = self.generate_one(toks[:,:start+1].contiguous(), toks_positions[:start+1], xenc, xenc_positions, T, top_k, repetition_penalty)[:,0] - 1
        # print(start, toks[:,:start+1], toks[:,start+1])
        with inference.inference_context():
            for i in it:
                toks[:,i+1] = self.generate_next(toks[:,i:i+1], toks_positions[i:i+1], xenc, xenc_positions, T, top_k, repetition_penalty)[:,0] - 1
                # print(i, N, toks[:,:i+1])
                if (toks[:,i+1] == -1).all(): return toks[:,1:i+1]

                # for profiling, debugging or early exit
                if step is not None: step()
        return toks[:,1:]
    
    @torch.no_grad()
    def generate_batch(self, txts, N=None, T=1.1, top_k=7, show_progress_bar=True):
        N = self.stoks_len
        dev = self.device
        ttoks = []
        for txt in txts:
            ttoks_ = torch.tensor(self.tokenizer.encode(txt), device=dev)
            ttoks_ = F.pad(ttoks_, (0, self.ttoks_len - len(ttoks_)), value=self.tokenizer.eot).unsqueeze(0)
            ttoks.append(ttoks_)
        ttoks = torch.cat(ttoks, dim=0)
        toks = torch.zeros((len(ttoks),N), dtype=torch.long, device=dev)
        it = range(N)
        if show_progress_bar: it = progress_bar(it)
        for i in it:
            p, _ = self(ttoks, toks[:,:i], loss=None)
            last_p = p[:,-1]
            if top_k:
                last_p[last_p < torch.topk(last_p, top_k).values[:,-1,None]] = -torch.inf
            tok = torch.multinomial((last_p / float(T)).softmax(-1), 1)
            toks[:,i] = tok[:,0]
            if (toks[:,i] == self.stoks_codes-1).all(): return toks[:,:i]
        return toks

# %% ../nbs/5B. Stoks transcriber.ipynb 45
def _make_model(size:str, tunables:Tunables=Tunables(), dataset=None, **kwargs):
    kwargs = dict(stoks_len = dataset.stoks_len,
                  ttoks_codes = dataset.ttoks_codes, ttoks_len = dataset.ttoks_len,
                  tokenizer = dataset.tokenizer,
                  tunables=tunables, **kwargs)
    if 'stoks_codes' not in kwargs: kwargs['stoks_codes'] = dataset.stoks_codes
    if size == 'micro':
        return TSARTransformer(depth=2, n_head=3, ffn_mult=1, **kwargs)
    if size == 'micro-wide':
        return TSARTransformer(depth=2, n_head=16, ffn_mult=6, **kwargs)
    if size == 'tiny':
        return TSARTransformer(depth=4, n_head=6, **kwargs)
    if size == 'base':
        return TSARTransformer(depth=6, n_head=8, **kwargs)
    if size == 'small':
        return TSARTransformer(depth=12, n_head=12, **kwargs)
    if size == 'small+':
        return TSARTransformer(depth=12, n_head=16, **kwargs)
    if size == 'medium':
        return TSARTransformer(depth=24, n_head=16, **kwargs)

def make_model(size:str, frozen_embeddings_model:str=None, tunables:Tunables=Tunables(), dataset:torch.utils.data.Dataset=None):
    from whisperspeech import vq_stoks

    model = _make_model(size, tunables, dataset)
    return model
