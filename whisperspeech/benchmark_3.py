# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/C. Benchmark 3.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/C. Benchmark 3.ipynb 2
import time
import torch
from fastcore.script import call_parse
from whisperspeech.pipeline import Pipeline
from whisperspeech import s2a_delar_mup_wds_mlang #, s2a_umup
from whisperspeech.inference import get_compute_device

from torch.profiler import profile, record_function, ProfilerActivity

# %% ../nbs/C. Benchmark 3.ipynb 3
from fastcore.script import anno_parser
import shlex

# watch out: we can only pass Python values as keyword arguments (not positional)
# everything else has to be a string
def parse_and_call(name, fun, args, kwargs={}, log_to_wandb=True):
    p = anno_parser(fun, prog=name)
    args = p.parse_args(shlex.split(args)).__dict__
    args.pop('xtra'); args.pop('pdb')
    args.update({k:v for k, v in kwargs.items()})
    return fun(**args)

# %% ../nbs/C. Benchmark 3.ipynb 4
def measure(fun, iterations = 10, step=lambda: None):
    ts = []
    for x in range(iterations):
        start = time.time()
        fun()
        getattr(torch, get_compute_device()).synchronize()
        ts.append(time.time() - start)
        step()
    ts = torch.tensor(ts)
    return ts.mean(), ts.std()

# def make_model(**kwargs):
#     kw = dict(quantizers=4, spk_width=192, stoks_codes=513, stoks_width=64,
#               depth=6, n_head=8)
#     kw.update(kwargs)
#     return SADelARTransformer(**kw).eval().cuda()
#     if size == 'micro':
#         return SADelARTransformer(depth=4, n_head=3, ffn_mult=2, **kwargs)
#     if size == 'tiny-narrow':
#         return SADelARTransformer(depth=4, n_head=6, ffn_mult=1, **kwargs)
#     if size == 'tiny':
#         return SADelARTransformer(depth=4, n_head=6, **kwargs)
#     if size == 'base':
#         return SADelARTransformer(depth=6, n_head=8, **kwargs)
#     if size == 'base-deep':
#         return SADelARTransformer(depth=9, n_head=8, **kwargs)
#     if size == 'base-wide':
#         return SADelARTransformer(depth=6, n_head=12, **kwargs)
#     if size == 'small/2':
#         return SADelARTransformer(depth=9, n_head=12, **kwargs)
#     if size == 'small':
#         return SADelARTransformer(depth=12, n_head=12, **kwargs)
#     if size == 'medium':
#         return SADelARTransformer(depth=24, n_head=16, **kwargs)

# experiments = {
#     depth = [1,2,3,4,5,6,7,8],
# }

@call_parse
def benchmark(
    args : str,
    tunables : str = "",
    batch_size : int = 1,
    max_seq_length : int = None,
    max_batch_size : int = None,
    no_torch_compile : bool = False,
    iterations = 10,
    umup = True,
    trace = False,
):
    mod = s2a_umup if umup else s2a_delar_mup_wds_mlang
#     kwargs = {kw:int(value) for arg in args.split(",") for kw,value in [arg.split('=', 1)]}
    tunables_txt = tunables
    tunables = parse_and_call('tunables', mod.Tunables, tunables, log_to_wandb=False)

    ms2a = mod._make_model(args, spk_width=192, tunables=tunables).eval().cuda()
    ms2a.optimize(max_batch_size=max_batch_size or batch_size, max_seq_length=max_seq_length, torch_compile=True)

    stoks = torch.zeros(250)
    t = len(stoks)/25
    
    def s2a():
        return ms2a.generate(stoks, Pipeline.default_speaker.unsqueeze(0), bs=batch_size, show_progress_bar=False)

    # warmup
    s2a()

    def trace_handler(p):
        table = prof.key_averages().table(sort_by="self_cpu_time_total", row_limit=10)
        # display(HTML(f'<pre style="white-space: pre; font-size: 80%;">{html.escape(table)}</pre>'))
        print(table)
        fname = f"trace-s2a-{args.replace(' ', '_')}-{tunables_txt.replace(' ', '_')}.json"
        print()
        print(f'https://jpc-ai-dev-1.tail3eb4a.ts.net/files/workspace/spear-tts-pytorch/{fname}')
        p.export_chrome_trace(fname)
        # display(HTML(f'<a href="{fname}" download>Download {fname}</a>'))

    if trace:
        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], 
                 on_trace_ready=trace_handler,
                 schedule=torch.profiler.schedule(
                    wait=0,
                    warmup=2,
                    active=1,
                    repeat=1,
                 ),
        ) as prof:
            s2a_mean, s2a_std = measure(s2a, iterations=iterations, step=prof.step)
    else:
        s2a_mean, s2a_std = measure(s2a, iterations=iterations)
    
    print(f"{args}\t{s2a_mean:.3f}\t{s2a_std:.3f}")
