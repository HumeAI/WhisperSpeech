# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/C. Benchmark 2.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/C. Benchmark 2.ipynb 2
import time
import torch
from fastcore.script import call_parse
from whisperspeech.pipeline import Pipeline
from whisperspeech.s2a_delar_mup_wds_mlang import SADelARTransformer, Tunables, _make_model
from whisperspeech.inference import get_compute_device

from torch.profiler import profile, record_function, ProfilerActivity

# %% ../nbs/C. Benchmark 2.ipynb 3
from fastcore.script import anno_parser
import shlex

# watch out: we can only pass Python values as keyword arguments (not positional)
# everything else has to be a string
def parse_and_call(name, fun, args, kwargs={}, log_to_wandb=True):
    p = anno_parser(fun, prog=name)
    args = p.parse_args(shlex.split(args)).__dict__
    args.pop('xtra'); args.pop('pdb')
    args.update({k:v for k, v in kwargs.items()})
    return fun(**args)

# %% ../nbs/C. Benchmark 2.ipynb 4
def measure(fun, iterations = 10, step=lambda: None):
    ts = []
    for x in range(iterations):
        start = time.time()
        fun()
        getattr(torch, get_compute_device()).synchronize()
        ts.append(time.time() - start)
        step()
    ts = torch.tensor(ts)
    return ts.mean(), ts.std()

@call_parse
def benchmark(
    args : str,
    tunables : str = "",
    batch_size : int = 1,
    stoks_len : int = 250,
    no_torch_compile : bool = False,
    trace : bool = False,
    iterations = 10,
):
    tunables = parse_and_call('tunables', Tunables, tunables, log_to_wandb=False)
    
    ms2a = _make_model(args, spk_width=192, tunables=tunables).eval().cuda()
    ms2a.optimize(max_batch_size=batch_size, torch_compile=True)

    stoks = torch.zeros(stoks_len)
    t = len(stoks)/25
    
    def s2a():
        return ms2a.generate(stoks, Pipeline.default_speaker.unsqueeze(0), bs=batch_size, show_progress_bar=False)

    # warmup
    measure(s2a, iterations=2)
    
    def trace_handler(p):
        table = prof.key_averages().table(sort_by="self_cpu_time_total", row_limit=10)
        print(table)
        fname = f"trace-s2a-{args.replace(' ', '_')}.json"
        print(fname)
        p.export_chrome_trace(fname)

    if trace:
        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], 
                 on_trace_ready=trace_handler,
                 schedule=torch.profiler.schedule(
                    wait=0,
                    warmup=2,
                    active=1,
                    repeat=1,
                 ),
        ) as prof:
            s2a_mean, s2a_std = measure(s2a, iterations=iterations, step=prof.step)
    else:
        s2a_mean, s2a_std = measure(s2a, iterations=iterations)

    print(f"{args}\t{s2a_mean:.3f}\t{s2a_std:.3f}")
