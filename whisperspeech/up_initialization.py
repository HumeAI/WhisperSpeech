# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/D. uP Initialization.ipynb.

# %% auto 0
__all__ = ['init_transformer']

# %% ../nbs/D. uP Initialization.ipynb 1
import torch
from torch import nn
from whisperspeech.modules import *

# %% ../nbs/D. uP Initialization.ipynb 2
track_parameters = True
track_buffers = False

# %% ../nbs/D. uP Initialization.ipynb 3
def init_transformer(self, m):
    lst = [] # a list to make sure we initialize every parameter
    
    bias_std = self.tunables.init_std
    def init_bias():
        if m.bias is not None:
            nn.init.trunc_normal_(m.bias, std=bias_std, a=-3*bias_std, b=3*bias_std); lst.append(m.bias)
    
    if isinstance(m, LinearHead):
        m.no_weight_decay = True
        nn.init.constant_(m.weight, 0); lst += [m.weight]
    elif isinstance(m, QueryHead):
        m.lr_scale = 1/(m.weight.shape[1] / self.base_width)
        nn.init.constant_(m.weight, 0); lst += [m.weight]
        init_bias()
    elif isinstance(m, nn.Embedding):
        m.no_weight_decay = True
        m.lr_scale = self.tunables.embeddings_lr_scale
        std = self.tunables.embeddings_std
        nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std); lst += [m.weight]
    elif isinstance(m, EmbeddingProjector):
        m.lr_scale = self.tunables.embedding_projector_lr_scale
        std = self.tunables.init_std
        nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std); lst += [m.weight]
        init_bias()
    elif isinstance(m, nn.Linear):
        m.lr_scale = 1/(m.weight.shape[1] / self.base_width)
        std = self.tunables.init_std / m.weight.shape[1]
        nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std); lst += [m.weight]
        if m.bias is not None:
            # My reading of the Tensor Program V paper seems to imply it should work but
            # empirically using unscaled stddev here results in a lot higher losses for T2S
            nn.init.trunc_normal_(m.bias, std=std, a=-3*std, b=3*std); lst += [m.bias]
    elif isinstance(m, nn.LayerNorm):
        m.no_weight_decay = True
        nn.init.constant_(m.bias, 0); lst += [m.bias]
        nn.init.constant_(m.weight, 1); lst += [m.weight]
        
    if track_parameters:
        lst = set(lst)
        for k,p in m.named_parameters(recurse=False):
            if p not in lst:
                print('missed param:', m, k)
    if track_buffers: # does not make sense
        for k,b in m.named_buffers(recurse=False):
            print('missed buffer:', m, k)
