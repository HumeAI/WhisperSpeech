# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Verify train dataset.ipynb.

# %% auto 0
__all__ = []

# %% ../nbs/Verify train dataset.ipynb 1
import webdataset as wds
from whisperspeech import vad_merge, utils

from pathlib import Path
from fastprogress import progress_bar
from fastcore.script import call_parse
from urllib.parse import urlparse
import json

# %% ../nbs/Verify train dataset.ipynb 2
@call_parse
def verify_train_dataset(dataset_dir:Path, prosmimic=False):
    pr = urlparse(str(dataset_dir))
    if pr.scheme == "":
        # local dataset
        with open(dataset_dir/'txt-samples.list') as f: samples = len(f.readlines())

        for name in ['small.en-txt', 'medium-txt']:
            if (dataset_dir/name).exists():
                txt_dir = name
                break
        assert txt_dir is not None, f"No transcripts found in {dataset_dir}"

        shards = utils.shard_glob(dataset_dir/txt_dir/'*.tar.gz')
    else:
        # remote dataset
        with wds.gopen(str(dataset_dir/'dataset.json')) as f: spec = json.load(f)
        samples = spec['samples']
        txt_dir = spec['txt_dir']
        shards = [str(dataset_dir/txt_dir/x)+'.gz' for x in spec['shards']]

    print(f"Found {len(shards)} shards")
    
    ds = wds.WebDataset(shards).compose(
        wds.decode(),
        utils.merge_in(utils.derived_dataset('stoks-raw-vq-medium-en+pl')),
    )
    if prosmimic: ds = ds.compose(utils.merge_in(utils.derived_dataset('prosmimic-raw'), missing_ok=True))
    ds = ds.compose(
        wds.to_tuple('__key__'),
        wds.batched(512),
    )
    dl = wds.WebLoader(ds, num_workers=16, batch_size=None).unbatched()
    for x in progress_bar(dl, total=samples):
        pass
